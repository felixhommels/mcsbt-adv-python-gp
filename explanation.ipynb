{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 1 Advanced Python Group Project Explanation Notebook\n",
    "This document explains step by step the actions taken in order to develop the first part of the Group Project: Flask. The overall structure of the notebook is a little explanation along with the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Repo: https://github.com/felixhommels/mcsbt-adv-python-gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonanywhere Link: flizerflix.pythonanywhere.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having found the dataset, the first step was to import all the libraries used for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From model.py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - After that since we are trying to make a machine learning model, we first had to load the data and drop colums, which dont add value to the model - in this case the Order_ID. \n",
    "- In order to make sure that we dont train on empty data, we also dropped all empty datapoints.\n",
    "- Since the dataset is working with categorical data and the aim was to use a linear regression, we used one-hot encoding or \"dummies\" to transform the data such that the Linear Regression would work with the dataset\n",
    "- Next we split the data into X and Y \n",
    "- Lastly we used scikitlearn to split the data into training and testing data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From model.py\n",
    "\n",
    "data = pd.read_csv(\"data/Food_Delivery_times.csv\")\n",
    "\n",
    "data.drop(columns=[\"Order_ID\"], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#We have some categorical variables - for linear regression we need to convert them to numerical variables\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "X = data.drop(columns=[\"Delivery_Time_min\"])\n",
    "Y = data[\"Delivery_Time_min\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we created a Linear Regression and fit the model to our X and Y training data respectively\n",
    "- In order to see what happened \"under the hood\", we examined the coefficients of each variable to better understand which variables were key for the prediction\n",
    "- We printed the feature name and coefficient respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From model.py\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "#During development wanted to see the coefficients of the different features\n",
    "coefficients = model.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "for feature, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we made the model predict Y based on our X testdata\n",
    "- We then measured the performance using mean squred error and R^2\n",
    "- During development, the R^2 of the model was approx. 0.83 which is an acceptable score\n",
    "- Lastly we saved the model into a pickle file which we could use in our Flask backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\") #During testing was approx .83 which is acceptable\n",
    "\n",
    "#Saving the model to pickle file\n",
    "pickle.dump(model, open(\"food_delivery_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Backend Routes & Templates for the Home Route and Predict Route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we needed to import all of the necessary libraries for a functioning backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then created variables which are responsible for setting the correct directories for the base file, pickle model and data model later used in the backend - this is vital for successful deployment later\n",
    "- We then created an app using Flask and configured it as DEBUG=True which updates the code \"live\" when changes are made while running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"models\", \"food_delivery_model.pkl\")\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"Food_Delivery_Times.csv\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config[\"DEBUG\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The home route was then created\n",
    "- The home route simply renders an HTML document in the front-end which gives some information on the endpoints such as: which route accepts which input parameters (query, path, body)\n",
    "- For the HTML code, please visit the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"welcome.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the prediction route was created. **This route accepts the parameters within the request body.** Note that this route is accessable through a template rendering front end as well as simple URL with a body - we wanted to keep the flexibility to have both. Lets dive deeper in the code:\n",
    "- This route accepts both GET and POST (where GET only renders the front-end form for the user to use and POST is used by the form as well as the URL to send the input data to the prediction model)\n",
    "- Next, the function checks if the request is post and determines if the front end was used or through a normal json body \n",
    "- All of the input parameters are retrieved from the data\n",
    "- The input data is formatted accordingly such that it can be loaded into a dataframe\n",
    "- The input dataframe (pandas), is reindexed so that the columns of the input_df match exactly with the ones the model was trained on\n",
    "- The prediction is then made\n",
    "- Since the output display differs based on if the front end was used or a json, the model determines whether to return a jsonify object or whether to render the \"output\" template\n",
    "- Again for the HTML, please visit the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "#Body parameter route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"POST\":\n",
    "        model = pickle.load(open(MODEL_PATH, \"rb\"))\n",
    "        \n",
    "        if request.is_json:\n",
    "            data = request.get_json()\n",
    "        else:\n",
    "            data = request.form\n",
    "\n",
    "        distance_km = data.get('Distance_km')\n",
    "        weather_clear = data.get('Weather_Clear')\n",
    "        weather_foggy = data.get('Weather_Foggy')\n",
    "        weather_rainy = data.get('Weather_Rainy')\n",
    "        weather_snowy = data.get('Weather_Snowy')\n",
    "        weather_windy = data.get('Weather_Windy')\n",
    "        traffic_level_low = data.get('Traffic_Level_Low')\n",
    "        traffic_level_medium = data.get('Traffic_Level_Medium')\n",
    "        traffic_level_high = data.get('Traffic_Level_High')\n",
    "        time_of_day_afternoon = data.get('Time_of_Day_Afternoon')\n",
    "        time_of_day_evening = data.get('Time_of_Day_Evening')\n",
    "        time_of_day_morning = data.get('Time_of_Day_Morning')\n",
    "        time_of_day_night = data.get('Time_of_Day_Night')\n",
    "        vehicle_type_bike = data.get('Vehicle_Type_Bike')\n",
    "        vehicle_type_car = data.get('Vehicle_Type_Car')\n",
    "        vehicle_type_scooter = data.get('Vehicle_Type_Scooter')\n",
    "        preparation_time_min = data.get('Preparation_Time_min')\n",
    "        courier_experience_yrs = data.get('Courier_Experience_yrs')\n",
    "\n",
    "        input_data = {\n",
    "            'Distance_km': [distance_km],\n",
    "            'Weather_Clear': [weather_clear],\n",
    "            'Weather_Foggy': [weather_foggy],\n",
    "            'Weather_Rainy': [weather_rainy],\n",
    "            'Weather_Snowy': [weather_snowy],\n",
    "            'Weather_Windy': [weather_windy],\n",
    "            'Traffic_Level_Low': [traffic_level_low],\n",
    "            'Traffic_Level_Medium': [traffic_level_medium],\n",
    "            'Traffic_Level_High': [traffic_level_high],\n",
    "            'Time_of_Day_Afternoon': [time_of_day_afternoon],\n",
    "            'Time_of_Day_Evening': [time_of_day_evening],\n",
    "            'Time_of_Day_Morning': [time_of_day_morning],\n",
    "            'Time_of_Day_Night': [time_of_day_night],\n",
    "            'Vehicle_Type_Bike': [vehicle_type_bike],\n",
    "            'Vehicle_Type_Car': [vehicle_type_car],\n",
    "            'Vehicle_Type_Scooter': [vehicle_type_scooter],\n",
    "            'Preparation_Time_min': [preparation_time_min],\n",
    "            'Courier_Experience_yrs': [courier_experience_yrs]\n",
    "        }\n",
    "\n",
    "        input_df = pd.DataFrame(input_data)\n",
    "        input_df = input_df.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "        prediction = model.predict(input_df)\n",
    "\n",
    "        if request.is_json:\n",
    "            return jsonify({\"predicted_delivery_time\": round(prediction[0], 2)})\n",
    "        else:\n",
    "            return render_template(\"prediction.html\", prediction=round(prediction[0], 2))\n",
    "\n",
    "    return render_template(\"prediction.html\", prediction=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the statistics route was created. **This route accepts query parameters.** Please note, that this route does not have a front-end and isnt using render template. Reason being that this is a GET only route and rendering the template requires a POST method. Lets dive into the code step-by-step:\n",
    "- First, the function gets the arguments and then loads the datafile from which it will later calculate the statistics\n",
    "- If courier_experience is given, it transforms the data into a float\n",
    "- There are then a variety of functions nested, each responsible for calculating statistics about certain weather, vehicle type, courier_experience, time of day or traffic \n",
    "- Note that since courier_experience is a number, we had to create bins of ranges to compute statistics around that\n",
    "- We instantiated a stats dictionary to accumulate statistics if there is more than one query parameter\n",
    "- If statements invoke the relevant statistics functions depending on the query parameters and add them to the stats dictionary\n",
    "- The stats get returned\n",
    "- *Note that you cannot query the same category twice (e.g. weather = \"clear\" and weather = \"foggy\" at the same time since it cant be both at time of delivery in the data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "@app.route(\"/statistics\", methods=[\"GET\"])\n",
    "def statistics():\n",
    "    # Get query parameters\n",
    "    weather = request.args.get('weather')\n",
    "    traffic = request.args.get('traffic')\n",
    "    time_of_day = request.args.get('time_of_day')\n",
    "    vehicle_type = request.args.get('vehicle_type')\n",
    "    courier_experience = request.args.get('courier_experience')\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    if courier_experience is not None:\n",
    "        try:\n",
    "            courier_experience = float(courier_experience)\n",
    "        except ValueError:\n",
    "            return jsonify({\"error\": \"Invalid value for courier_experience. Must be a number.\"}), 400\n",
    "\n",
    "    def get_weather_stats(data, weather):\n",
    "        if weather:\n",
    "            data = data[data['Weather'] == weather.title()]\n",
    "        return {\n",
    "            \"average_distance_km\": data[\"Distance_km\"].mean(),\n",
    "            \"average_preparation_time_min\": data[\"Preparation_Time_min\"].mean(),\n",
    "            \"average_courier_experience_yrs\": data[\"Courier_Experience_yrs\"].mean(),\n",
    "            \"average_delivery_time_min\": data[\"Delivery_Time_min\"].mean()\n",
    "        }\n",
    "\n",
    "    def get_traffic_stats(data, traffic):\n",
    "        if traffic:\n",
    "            data = data[data['Traffic_Level'] == traffic.title()]\n",
    "        return {\n",
    "            \"average_distance_km\": data[\"Distance_km\"].mean(),\n",
    "            \"average_preparation_time_min\": data[\"Preparation_Time_min\"].mean(),\n",
    "            \"average_courier_experience_yrs\": data[\"Courier_Experience_yrs\"].mean(),\n",
    "            \"average_delivery_time_min\": data[\"Delivery_Time_min\"].mean()\n",
    "        }\n",
    "\n",
    "    def get_time_of_day_stats(data, time_of_day):\n",
    "        if time_of_day:\n",
    "            data = data[data['Time_of_Day'] == time_of_day.title()]\n",
    "        return {\n",
    "            \"average_distance_km\": data[\"Distance_km\"].mean(),\n",
    "            \"average_preparation_time_min\": data[\"Preparation_Time_min\"].mean(),\n",
    "            \"average_courier_experience_yrs\": data[\"Courier_Experience_yrs\"].mean(),\n",
    "            \"average_delivery_time_min\": data[\"Delivery_Time_min\"].mean()\n",
    "        }\n",
    "\n",
    "    def get_vehicle_type_stats(data, vehicle_type):\n",
    "        if vehicle_type:\n",
    "            data = data[data['Vehicle_Type'] == vehicle_type.title()]\n",
    "        return {\n",
    "            \"average_distance_km\": data[\"Distance_km\"].mean(),\n",
    "            \"average_preparation_time_min\": data[\"Preparation_Time_min\"].mean(),\n",
    "            \"average_courier_experience_yrs\": data[\"Courier_Experience_yrs\"].mean(),\n",
    "            \"average_delivery_time_min\": data[\"Delivery_Time_min\"].mean()\n",
    "        }\n",
    "\n",
    "    def get_courier_experience_stats(data, courier_experience):\n",
    "        data[\"Courier_Experience_Group\"] = pd.cut(data[\"Courier_Experience_yrs\"], bins=[0, 1, 3, 5, 10], labels=[\"0-1\", \"1-3\", \"3-5\", \"5-10\"])\n",
    "        if courier_experience is not None:\n",
    "            if 0 < courier_experience <= 1:\n",
    "                experience_group = \"0-1\"\n",
    "            elif 1 < courier_experience <= 3:\n",
    "                experience_group = \"1-3\"\n",
    "            elif 3 < courier_experience <= 5:\n",
    "                experience_group = \"3-5\"\n",
    "            elif 5 < courier_experience <= 10:\n",
    "                experience_group = \"5-10\"\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "            data = data[data[\"Courier_Experience_Group\"] == experience_group]\n",
    "            return {\n",
    "                \"average_distance_km\": data[\"Distance_km\"].mean(),\n",
    "                \"average_preparation_time_min\": data[\"Preparation_Time_min\"].mean(),\n",
    "                \"average_delivery_time_min\": data[\"Delivery_Time_min\"].mean()\n",
    "            }\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    if weather:\n",
    "        w_stats = get_weather_stats(data, weather)\n",
    "        stats[f\"weather_stats_{weather}\"] = w_stats\n",
    "    if traffic:\n",
    "        t_stats = get_traffic_stats(data, traffic)\n",
    "        stats[f\"traffic_stats_{traffic}\"] = t_stats\n",
    "    if time_of_day:\n",
    "        tod_stats = get_time_of_day_stats(data, time_of_day)\n",
    "        stats[f\"time_of_day_stats_{time_of_day}\"] = tod_stats\n",
    "    if vehicle_type:\n",
    "        v_stats = get_vehicle_type_stats(data, vehicle_type)\n",
    "        stats[f\"vehicle_type_stats_{vehicle_type}\"] = v_stats\n",
    "    if courier_experience is not None:\n",
    "        c_stats = get_courier_experience_stats(data, courier_experience)\n",
    "        stats[f\"courier_experience_stats_{courier_experience}\"] = c_stats\n",
    "\n",
    "    return jsonify(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we created the \"Order ID\" Route. **This is the path parameter route**. It allows users to fetch information of a certain order. \n",
    "- Only a GET route and expects the parameters in the path\n",
    "- The data gets loaded and we filter the data based on the Order_ID which was provided in the path\n",
    "- Lastly, we return a jsonify object with the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "#Path parameter route\n",
    "@app.route(\"/data/<int:order_id>\", methods=[\"GET\"])\n",
    "def data(order_id):\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "    data['Order_ID'] = data['Order_ID'].astype(int)\n",
    "    return jsonify(data[data['Order_ID'] == order_id].to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webhook and CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we created a webhook function which allows us to implement a CI/CD pipeline for Pythonanywhere deployment. It handles the incoming webhook notifications from GitHub and automatically pulls the changes from the repo when changes are made. The route only has a post method. More details:\n",
    "- First we had to provide the paths to the repo in Pythonanywhere as well as the WSGI file\n",
    "- It checks if the incoming request is json since webhook payloads are json format\n",
    "- If its valid, it then extracts the repo name\n",
    "- Then the function changes directory to where the repo and files are stored \n",
    "- The code then executes a git pull which pulls the changes from GitHub and \"restarts\" the WSGI by using the touch keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From app.py\n",
    "\n",
    "#Webhook route\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def webhook():\n",
    "    path_repo = \"/home/flizerflix/mcsbt-adv-python-gp\"\n",
    "    servidor_web = \"/var/www/flizerflix_pythonanywhere_com_wsgi.py\"\n",
    "\n",
    "    if request.is_json:\n",
    "        payload = request.json\n",
    "\n",
    "        if \"repository\" in payload:\n",
    "            repo_name = payload[\"repository\"][\"name\"]\n",
    "\n",
    "            try:\n",
    "                os.chdir(path_repo)\n",
    "            except FileNotFoundError:\n",
    "                return jsonify({\"message\": \"The directory of the repository does not exist!\"}), 404\n",
    "\n",
    "            try:\n",
    "                subprocess.run([\"git\", \"pull\"], check=True)\n",
    "                subprocess.run([\"touch\", servidor_web], check=True)\n",
    "                return jsonify({\"message\": f\"Successfully pulled latest changes for {repo_name}\"}), 200\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                return jsonify({\"message\": f\"Git pull failed!\", \"error\": str(e)}), 500\n",
    "        else:\n",
    "            return jsonify({\"message\": \"No repository information in payload\"}), 400\n",
    "    else:\n",
    "        return jsonify({\"message\": \"Invalid request, expected JSON\"}), 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see that the last two pushes were successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ![Alt text](webhook_proof.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
